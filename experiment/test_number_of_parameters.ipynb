{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import os, platform, sys, operator as op\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import theano_models as tm\n",
    "import theano_models.deterministic_models as dm\n",
    "import theano_models.probabilistic_models as pm\n",
    "import warnings\n",
    "from experiment_util import track, hyper_init_random, hyper_init_dict\n",
    "from schlichtanders.mycontextmanagers import ignored\n",
    "import csv\n",
    "from ast import literal_eval\n",
    "import random\n",
    "\n",
    "from sqlalchemy import Column, Integer, Unicode, UnicodeText, String, PickleType, Float, Boolean\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "\n",
    "inf = float(\"inf\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "tm.inputting_references.update(['to_be_randomized'])\n",
    "\n",
    "__file__ = os.path.realpath('__file__')\n",
    "if platform.system() == \"Windows\":\n",
    "    from schlichtanders.myos import replace_unc\n",
    "    __file__ = replace_unc(__file__)\n",
    "__path__ = os.path.dirname(__file__)\n",
    "__parent__ = os.path.dirname(__path__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def toy_likelihood():\n",
    "    x = tm.as_tensor_variable([0.5])  #T.vector()\n",
    "    y = x + 0.3 * T.sin(2*np.pi*x)\n",
    "    func = tm.Model(inputs=[x], outputs=y, name=\"sin\")\n",
    "    return tm.Merge(pm.GaussianNoise(y, init_var=0.001), func, ignore_references={'parameters', 'parameters_positive'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# defaults:\n",
    "foldername = \"test\"\n",
    "filename = \"toy_example_test\"\n",
    "#overwrite as far as given:\n",
    "if len(sys.argv) > 2:\n",
    "    foldername, filename = sys.argv[1:3]\n",
    "elif len(sys.argv) > 1:\n",
    "    foldername = sys.argv[1]\n",
    "\n",
    "with ignored(OSError):\n",
    "    os.mkdir(os.path.join(__path__, foldername))\n",
    "filepath = os.path.join(__path__, foldername, '%s.db' % filename)\n",
    "errorfilepath = os.path.join(__path__, foldername, '%s_errors.txt' % filename)\n",
    "csvpath = os.path.join(__path__, 'good_parameters.csv')\n",
    "\n",
    "\n",
    "model_names = { # sorted by optimization type\n",
    "    \"ml\": ['baselinedet'],\n",
    "    \"annealing\": ['baseline', 'mixture', 'planarflow', 'planarflowdet', 'radialflow', 'radialflowdet'],\n",
    "}\n",
    "model_prefixes = reduce(op.add, model_names.values())\n",
    "model_prefixes = [p+\"_\" for p in model_prefixes]\n",
    "\n",
    "# Hyperparameters\n",
    "# ===============\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "class Hyper(Base):\n",
    "    __tablename__ = \"hyper\"\n",
    "    id = Column(Integer, primary_key=True)\n",
    "\n",
    "    # hyper parameters:\n",
    "    x_true = Column(Float)\n",
    "    max_epochs_without_improvement = Column(Integer)\n",
    "    logP_average_n = Column(Integer)\n",
    "    errorrate_average_n = Column(Integer)\n",
    "    minus_log_s1 = Column(Integer)\n",
    "    minus_log_s2 = Column(Integer)\n",
    "    batch_size = Column(Integer)\n",
    "\n",
    "    n_normflows = Column(Integer)\n",
    "\n",
    "    opt_identifier = Column(String)\n",
    "    opt_momentum = Column(Float)\n",
    "    opt_offset = Column(Float)\n",
    "    opt_decay = Column(Float)\n",
    "    opt_step_rate = Column(Float)\n",
    "\n",
    "    for _prefix in model_prefixes:\n",
    "        exec(\"\"\"\n",
    "{0}best_val_loss = Column(Float)\n",
    "{0}best_parameters = Column(PickleType, nullable=True)\n",
    "{0}train_loss = Column(PickleType)\n",
    "{0}val_loss = Column(PickleType)\n",
    "{0}epochs = Column(Integer)\n",
    "{0}init_params = Column(PickleType, nullable=True)\n",
    "{0}val_error_rate = Column(Float)\"\"\".format(_prefix))\n",
    "    def __init__(self, x_true):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        datasetname : str\n",
    "        \"\"\"\n",
    "        self.x_true = x_true\n",
    "        self.max_epochs_without_improvement = 30\n",
    "        self.logP_average_n = 3  # TODO random.choice([1,10])\n",
    "        self.errorrate_average_n = 10\n",
    "        self.init_results()\n",
    "\n",
    "    def init_results(self):\n",
    "\n",
    "        # extra for being able to reset results for loaded hyperparameters\n",
    "        for prefix in model_prefixes:\n",
    "            setattr(self, prefix + \"best_parameters\", None)\n",
    "            setattr(self, prefix + \"best_val_loss\", inf)\n",
    "            setattr(self, prefix + \"train_loss\", [])\n",
    "            setattr(self, prefix + \"val_loss\", [])\n",
    "            setattr(self, prefix + \"best_epoch\", 0)\n",
    "            setattr(self, prefix + \"init_params\", None)\n",
    "            setattr(self, prefix + \"val_error_rate\", inf)\n",
    "\n",
    "engine = create_engine('sqlite:///' + filepath)  # os.path.join(__path__, foldername, '%s.db' % filename)\n",
    "Base.metadata.create_all(engine)\n",
    "Session = sessionmaker(bind=engine)\n",
    "sql_session = Session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "good_parameters = []\n",
    "with open(csvpath, \"r\") as f:\n",
    "    reader = csv.DictReader(f, quoting=csv.QUOTE_NONE)\n",
    "    for row in reader:\n",
    "        # this should not be updated in hyper\n",
    "        # (double quote as quoting=csv.QUOTE_NONNUMERIC was used to create this csv)\n",
    "        del row['\"datasetname\"']\n",
    "        # TODO if the first runs through, delete also n_normflows, as we want to have all of them\n",
    "        good_parameters.append(\n",
    "            {literal_eval(k): literal_eval(v) for k, v in row.iteritems()}\n",
    "        )  # evaluate everything, this version also distinguishes ints/floats\n",
    "random.shuffle(good_parameters)  # permutes inplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 -> 5\n",
      "2 -> 8\n",
      "3 -> 11\n",
      "4 -> 14\n",
      "8 -> 26\n",
      "20 -> 62\n"
     ]
    }
   ],
   "source": [
    "for n_normflows in [1,2,3,4,8,20]:\n",
    "    x_true = 0.63\n",
    "    params = good_parameters[0]\n",
    "    hyper = Hyper(x_true)\n",
    "    hyper_init_random(hyper)\n",
    "    hyper_init_dict(hyper, params)\n",
    "    hyper.n_normflows = n_normflows\n",
    "\n",
    "    targets = toy_likelihood()\n",
    "    total_size = tm.total_size(targets['inputs'])\n",
    "    params_base = pm.DiagGauss(output_size=total_size)\n",
    "    normflows = [dm.PlanarTransform() for _ in range(hyper.n_normflows)]\n",
    "    # LocScaleTransform for better working with PlanarTransforms\n",
    "    params = params_base\n",
    "    for transform in normflows:\n",
    "        params = tm.normalizing_flow(transform, params)  # returns transform, however with adapted logP  # TODO merge does not seem to work correctly\n",
    "\n",
    "    prior = tm.fix_params(pm.DiagGauss(output_size=total_size))\n",
    "    model = tm.variational_bayes(targets, 'inputs', params, priors=prior)\n",
    "    loss = tm.loss_variational(model)\n",
    "\n",
    "    # all_params = tm.prox_reparameterize(model['parameters_positive'], tm.softplus, tm.softplus_inv)\n",
    "    all_params = tm.prox_reparameterize(model['parameters_positive'], track.squareplus, track.squareplus_inv)\n",
    "    all_params += model['parameters']\n",
    "    flat = tm.prox_flatten(tm.prox_center(all_params))\n",
    "    print \"%i -> %i\" %(n_normflows, len(flat.eval()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 4,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
