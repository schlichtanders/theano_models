{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gnumpy: failed to import cudamat. Using npmat instead. No GPU will be used.\n"
     ]
    }
   ],
   "source": [
    "from matplotlib.pyplot import plot, ylabel, xlabel, yscale, xscale, legend, subplots, gca, gcf\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from theano import function\n",
    "import numpy as np\n",
    "import gzip\n",
    "import cPickle\n",
    "from scipy.optimize import minimize\n",
    "from climin.util import optimizer\n",
    "from itertools import repeat, cycle, islice, izip\n",
    "import random\n",
    "inf = float(\"inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from breze.learn.data import one_hot\n",
    "from breze.learn.base import cast_array_to_local_type\n",
    "from schlichtanders.myfunctools import compose, meanmap, summap, compose_fmap, Average\n",
    "from schlichtanders import myfunctools\n",
    "from schlichtanders.mygenerators import eatN, chunk, chunk_list, every, takeN\n",
    "from schlichtanders.myplot import add_val, add_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from theano_models import (Merge, Flatten, Reparameterize, reduce_all_identities,\n",
    "                           inputting_references, outputting_references)\n",
    "from theano_models.tools import (as_tensor_variable, total_size, clone, clone_all,PooledRandomStreams,\n",
    "                                 get_profile, squareplus, squareplus_inv, softplus, softplus_inv)\n",
    "from theano_models.visualization import d3viz\n",
    "from IPython.display import IFrame\n",
    "import theano_models.deterministic_models as dm\n",
    "import theano_models.probabilistic_models as pm\n",
    "import theano_models.extra_models as em\n",
    "import theano_models.postmaps as post\n",
    "from theano_models.composing import normalizing_flow, variational_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.printing import debugprint\n",
    "from theano.tensor.shared_randomstreams import RandomStreams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import Column, Integer, Unicode, UnicodeText, String, PickleType, Float\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.ext.declarative import declarative_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'flat',\n",
       "  'inputs',\n",
       "  'n_data',\n",
       "  'noise',\n",
       "  'parameters',\n",
       "  'parameters_positive',\n",
       "  'parameters_psumto1',\n",
       "  'to_be_randomized'},\n",
       " {'kl_prior', 'logP', 'loglikelihood', 'norm_det', 'outputs'})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputting_references.update(['to_be_randomized'])\n",
    "inputting_references, outputting_references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from schlichtanders.myobjects import NestedNamespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pm.RNG = NestedNamespace(PooledRandomStreams(pool_size=int(5e8)), RandomStreams())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(50000, 784),\n",
       " (50000, 10),\n",
       " (10000, 784),\n",
       " (10000, 10),\n",
       " (10000, 784),\n",
       " (10000, 10)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datafile = '../data/mnist.pkl.gz'\n",
    "# Load data.        a                                                                                           \n",
    "\n",
    "with gzip.open(datafile,'rb') as f:                                                                        \n",
    "    train_set, val_set, test_set = cPickle.load(f)                                                       \n",
    "\n",
    "X, Z = train_set                                                                                               \n",
    "VX, VZ = val_set\n",
    "TX, TZ = test_set\n",
    "\n",
    "Z = one_hot(Z, 10)\n",
    "VZ = one_hot(VZ, 10)\n",
    "TZ = one_hot(TZ, 10)\n",
    "\n",
    "# from UncertainWeightsPaper rescaled the data,\n",
    "# however it already seems to be normalized\n",
    "# X /= 126  \n",
    "# VX /= 126\n",
    "# TX /= 126\n",
    "# X *= 2\n",
    "# VX *= 2\n",
    "# TX *= 2\n",
    "\n",
    "image_dims = 28, 28\n",
    "\n",
    "X, Z, VX, VZ, TX, TZ = [cast_array_to_local_type(i) for i in (X, Z, VX,VZ, TX, TZ)]\n",
    "map(np.shape, [X, Z, VX, VZ, TX, TZ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState at 0x7f50957c5d90>,\n",
       " 'average_n': 1,\n",
       " 'batch_size': 128,\n",
       " 'best_parameters': None,\n",
       " 'best_val_loss': inf,\n",
       " 'mapreduce': 'summap',\n",
       " 'minus_log_s1': 0,\n",
       " 'minus_log_s2': 6,\n",
       " 'n_epochs': 20,\n",
       " 'n_normflows': 4,\n",
       " 'opt_decay': 0.8047322013410626,\n",
       " 'opt_identifier': 'rmsprop',\n",
       " 'opt_momentum': 0.007177364176257046,\n",
       " 'opt_offset': 4.326533864677926e-05,\n",
       " 'opt_step_rate': 0.0001,\n",
       " 'pi': 0.75,\n",
       " 'train_loss': [],\n",
       " 'units_per_layer': 800,\n",
       " 'val_loss': []}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine = create_engine('sqlite:///hyperparameters.db')\n",
    "Base = declarative_base(bind=engine)\n",
    "\n",
    "class RandomHyper(Base):\n",
    "    __tablename__ = 'hyper'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    \n",
    "    # hyper parameters:\n",
    "    n_epochs = Column(Integer)\n",
    "    batch_size = Column(Integer)\n",
    "    mapreduce = Column(String)\n",
    "    average_n = Column(Integer)\n",
    "    units_per_layer = Column(Integer)\n",
    "    pi = Column(Float)\n",
    "    minus_log_s1 = Column(Integer)\n",
    "    minus_log_s2 = Column(Integer)\n",
    "    \n",
    "    n_normflows = Column(Integer)\n",
    "    \n",
    "    opt_identifier = Column(String)\n",
    "    opt_momentum = Column(Float)\n",
    "    opt_offset = Column(Float)\n",
    "    opt_decay = Column(Float)\n",
    "    opt_step_rate = Column(Float)\n",
    "    \n",
    "    # results:\n",
    "    best_val_loss = Column(Float)\n",
    "    best_parameters = Column(PickleType, nullable=True)\n",
    "    train_loss = Column(PickleType)\n",
    "    val_loss = Column(PickleType)\n",
    "\n",
    "    def __init__(self):\n",
    "        # hyper parameters:\n",
    "        self.n_epochs = 20\n",
    "        self.batch_size = 128\n",
    "        self.mapreduce = random.choice([\"summap\", \"meanmap\"])\n",
    "        self.average_n = 1\n",
    "        self.units_per_layer = np.random.choice([400, 800, 1200], p=[0.5, 0.3, 0.2])\n",
    "        self.pi = random.choice([1/4, 1/2, 3/4])\n",
    "        self.minus_log_s1 = random.choice([0,1,2])\n",
    "        self.minus_log_s2 = random.choice([6,7,8])\n",
    "        \n",
    "        self.n_normflows = random.choice([1,2,3,4,8,32])\n",
    "        \n",
    "        self.opt_identifier = random.choice([\"adadelta\", \"adam\", \"rmsprop\"])\n",
    "        if self.opt_identifier == \"adadelta\":\n",
    "            self.opt_momentum = random.choice([np.random.uniform(0, 0.01), np.random.uniform(0.9, 1)])\n",
    "            self.opt_offset = random.choice([5e-5, 1e-8])\n",
    "            self.opt_step_rate = random.choice([1, 1e-3, 1e-4, 1e-5])\n",
    "        elif self.opt_identifier == \"adam\":\n",
    "            self.opt_momentum = random.choice([np.random.uniform(0, 0.01), np.random.uniform(0.8, 0.93)])\n",
    "            self.opt_offset = 10 ** -np.random.uniform(3, 4)\n",
    "            self.opt_step_rate = random.choice([1e-3, 1e-4, 1e-5])\n",
    "        elif self.opt_identifier == \"rmsprop\":\n",
    "            self.opt_momentum = random.choice([np.random.uniform(0.002, 0.008), np.random.uniform(0.9, 1)])\n",
    "            self.opt_offset = np.random.uniform(0, 0.000045)\n",
    "            self.opt_step_rate = random.choice([1e-3, 1e-4, 1e-5])\n",
    "        self.opt_decay = np.random.uniform(0.78, 1)\n",
    "        \n",
    "        self.init_results()\n",
    "    \n",
    "    def init_results(self):\n",
    "        # extra for being able to reset results for loaded hyperparameters\n",
    "        self.best_parameters = None\n",
    "        self.best_val_loss = inf\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "\n",
    "Base.metadata.create_all()\n",
    "Session = sessionmaker(bind=engine)\n",
    "sql_session = Session()\n",
    "hyper = RandomHyper()\n",
    "hyper.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data modelling"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# for continous supervised data:\n",
    "predictor = dm.Mlp(output_size=10, output_transfer=\"softmax\", hidden_sizes=[200]*2, hidden_transfers=[\"rectifier\"]*2)\n",
    "post.flatten_parameters(predictor)\n",
    "\n",
    "target_distribution = pm.DiagGaussianNoise(predictor)\n",
    "\n",
    "targets = Merge(target_distribution, inputs=predictor['inputs'], to_be_randomized=predictor['parameters_flat'])\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is extremely useful to tell everything the default sizes\n",
    "input = as_tensor_variable(X[0], name=\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictor = dm.Mlp(\n",
    "    input=input,\n",
    "    output_size=Z.shape[1],\n",
    "    output_transfer=\"softmax\",\n",
    "    hidden_sizes=[hyper.units_per_layer]*2,\n",
    "    hidden_transfers=[\"rectifier\"]*2\n",
    ")\n",
    "predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_distribution = pm.Categorical(predictor)\n",
    "target_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "targets = Merge(target_distribution, predictor,\n",
    "                Flatten(predictor['parameters'], flat_key=\"to_be_randomized\")) #givens={predictor['inputs'][0]: X[0]}\n",
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parameter modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params_base = pm.DiagGauss(output_size=total_size(targets['to_be_randomized']))  \n",
    "# if you want to use size directly, CAUTION, you need to copy before!\n",
    "# params_base.map('parameters_positive', reparameterize_map(squareplus, squareplus_inv), 'parameters')\n",
    "params_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normflows = [dm.PlanarTransform() for _ in range(hyper.n_normflows)]\n",
    "normflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = params_base\n",
    "for transform in normflows:\n",
    "    params = normalizing_flow(transform, params)  # returns transform, however with adapted logP    \n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g1 = pm.Gauss(total_size(targets['to_be_randomized']), init_var=np.exp(-2* hyper.minus_log_s1))\n",
    "g2 = pm.Gauss(total_size(targets['to_be_randomized']), init_var=np.exp(-2* hyper.minus_log_s2))\n",
    "prior = pm.Mixture(g1, g2, mixture_probs=[hyper.pi, 1-hyper.pi])\n",
    "# label hyper parameters accordingly\n",
    "prior = Merge(prior,\n",
    "              parameters=None, # mean is not adapted at all, but left centred at zero\n",
    "              parameters_positive='hyperparameters_positive',\n",
    "              parameters_psumto1='hyperparameters_psumto1')\n",
    "prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = variational_bayes(targets, 'to_be_randomized', params, priors=prior)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model = Merge(model, Reparameterize(model['parameters_positive'], squareplus, squareplus_inv))\n",
    "# UncertainWeights uses softplus parameterization\n",
    "model = Merge(model, Reparameterize(model['parameters_positive'], softplus, softplus_inv))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Merge(model, Flatten(model['parameters']))\n",
    "#               em.NoisePool(model['noise']) #, givens={predictor['inputs'][0]: X[0]}\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dm.InvertibleModel.INVERTIBLE_MODELS\n",
    "reduce_all_identities()\n",
    "# dm.InvertibleModel.INVERTIBLE_MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Climin wants an iterator of (args, kwarsg) as keyword argument \"args\" (to be passed to the loss function). Concretley, we use an infinite iterator over minibatches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_batches = X.shape[0] // hyper.batch_size  # after this many steps we went through the whole data set once\n",
    "\n",
    "climin_args = izip(izip(chunk(hyper.batch_size, cycle(Z)), chunk(hyper.batch_size, cycle(X))), repeat({}))\n",
    "\n",
    "def weights_regularizer_1epoch():\n",
    "    for i in range(1, n_batches+1):\n",
    "        yield 2**(n_batches - i) / (2**n_batches - 1)\n",
    "        \n",
    "assert len(list(weights_regularizer_1epoch())) == n_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mapreduce = getattr(myfunctools, hyper.mapreduce)\n",
    "if hyper.average_n > 1:\n",
    "    mapreduce = compose_fmap(Average(hyper.average_n), mapreduce)\n",
    "\n",
    "postmap = compose(post.flat_numericalize_postmap, post.variational_postmap) \n",
    "postmap_kwargs = {\n",
    "    'mapreduce': mapreduce,  # TODO add more functionality for composed fmaps, with args\n",
    "    'annealing_combiner': post.AnnealingCombiner(\n",
    "        weights_regularizer=cycle(weights_regularizer_1epoch())\n",
    "    ),\n",
    "    'adapt_init_params': lambda ps: ps + np.random.normal(size=ps.size, scale=0.01),\n",
    "#     'profile': True,\n",
    "    'mode': 'FAST_RUN'\n",
    "}\n",
    "optimizer_kwargs = postmap(model, **postmap_kwargs)\n",
    "climin_kwargs = post.climin_postmap(optimizer_kwargs)\n",
    "climin_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "opt = optimizer(\n",
    "    identifier=hyper.opt_identifier,\n",
    "    step_rate=hyper.opt_step_rate,\n",
    "    momentum=hyper.opt_momentum,\n",
    "    decay=hyper.opt_decay,\n",
    "    offset=hyper.opt_offset,\n",
    "    \n",
    "    args=climin_args,\n",
    "    **climin_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualized Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot: \n",
    "line_train, = plot([], [], 'go-', label=\"average training loss\")\n",
    "line_curr_val, = plot([],[], 'bo:', label=\"avrg current validation loss\")\n",
    "line_best_val, = plot([], [], 'ko-', label=\"avrg best validation loss\")\n",
    "# plt.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "yscale('log')\n",
    "gca().yaxis.set_minor_formatter(FormatStrFormatter(\"%.2g\"))\n",
    "ylabel(\"validation loss\")\n",
    "xlabel(\"#epoch\")\n",
    "legend(loc='lower left', fancybox=True, framealpha=0.5)\n",
    "\n",
    "# start values:\n",
    "val_loss = optimizer_kwargs['num_loss'](optimizer_kwargs['num_parameters'], VZ, VX, no_annealing=True)\n",
    "training_loss = optimizer_kwargs['num_loss'](optimizer_kwargs['num_parameters'], Z[:10], X[:10], no_annealing=True)\n",
    "start_epoch = opt.n_iter//n_batches\n",
    "\n",
    "add_point(line_best_val, start_epoch, val_loss)\n",
    "add_point(line_curr_val, start_epoch, val_loss)\n",
    "add_point(line_train, start_epoch, training_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for info in takeN(hyper.n_epochs, every(n_batches, opt)):\n",
    "    current_epoch = info['n_iter']//n_batches\n",
    "    # collect and visualize validation loss for choosing the best model\n",
    "    val_loss = optimizer_kwargs['num_loss'](opt.wrt, VZ, VX, no_annealing=True)\n",
    "    if val_loss < hyper.best_val_loss:\n",
    "        hyper.best_parameters = opt.wrt\n",
    "        hyper.best_val_loss = val_loss\n",
    "        add_point(line_best_val, current_epoch, val_loss)\n",
    "    hyper.val_loss.append(val_loss)\n",
    "    add_point(line_curr_val, current_epoch, val_loss)\n",
    "    \n",
    "    # visualize training loss for comparison:\n",
    "    training_loss = optimizer_kwargs['num_loss'](opt.wrt, Z[:10], X[:10], no_annealing=True)\n",
    "    hyper.train_loss.append(training_loss)\n",
    "    add_point(line_train, current_epoch, training_loss)\n",
    "    \n",
    "#     print info['n_iter'], training_loss, val_loss, best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sql_session.add(hyper)\n",
    "sql_session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: average over predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print best_val_loss\n",
    "model['flat'] = best_wrt\n",
    "\n",
    "predict = mlp.function()\n",
    "predict(X[0]), Z[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PX = np.apply_along_axis(predict, 1, X)\n",
    "PVX = np.apply_along_axis(predict, 1, VX)\n",
    "PTX = np.apply_along_axis(predict, 1, TX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'incorrect samples train/val/test:  %i/%i/%i' % (\n",
    "    (PX[:, :10].argmax(1) != Z.argmax(1)).sum(),\n",
    "    (PVX[:, :10].argmax(1) != VZ.argmax(1)).sum(),\n",
    "    (PTX[:, :10].argmax(1) != TZ.argmax(1)).sum())\n",
    "\n",
    "print 'error rate train/val/test:  %g/%g/%g' % (\n",
    "    (PX[:, :10].argmax(1) != Z.argmax(1)).mean(),\n",
    "    (PVX[:, :10].argmax(1) != VZ.argmax(1)).mean(),\n",
    "    (PTX[:, :10].argmax(1) != TZ.argmax(1)).mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 4.0,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}